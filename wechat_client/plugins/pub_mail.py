import xmltodict
import logging
import arxiv
import traceback
import concurrent.futures
from datetime import timedelta


from config import MAX_PUBLIC_NUM
from plugins.article import get_article_from_pub
from database.sql_database import (
    get_user_subscription_info,
    insert_token_usage,
    insert_wx_article_to_sql,get_user_all_pub_id,
    get_last_user_pub_push_time,get_article_by_pub_id_and_push_time,
    delete_user_pub_by_name,
    update_user_push_status,
    get_user_subscription_info,
    get_arxiv_by_id, insert_arxiv,get_github_trending_by_name,upsert_github_trending
    )

from prompt import ARXIV_SYSTEM
from utils.llm import call_llm,SystemMessage,UserMessage,Messages
from utils.misc import try_extract_json,get_now,article_summary,translate
from utils.github_trending import parse_github_trending,get_github_content
from const.enums import UseCase



def get_pub_push(user_id, max_cnt=100):
    # è·å–å…³æ³¨çš„å…¬ä¼—å·
    pub_ids = get_user_all_pub_id(user_id)
    if not pub_ids:
        return []
    # è·å–ä¸Šæ¬¡å‘é€çš„æ—¶é—´
    last_push_time = get_last_user_pub_push_time(user_id)
    # å¦‚æœæ²¡æœ‰å‘é€è¿‡ï¼Œé»˜è®¤ä½¿ç”¨ä¸€å‘¨çš„æ—¶é—´
    if not last_push_time:
        last_push_time = get_now() - timedelta(days=7)
    # è·å–ä»ä¸Šæ¬¡å‘é€åˆ°ç°åœ¨çš„å…¬ä¼—å·æ–‡ç« 
    docs = []
    for pub_id in pub_ids.values():
        docs.extend(get_article_by_pub_id_and_push_time(pub_id['pub_id'],last_push_time))
    # èšåˆï¼ŒæŠ½å–ä¸»é¢˜
    if not docs:
        return []
    docs = docs[:max_cnt]
    return docs

async def get_github_trending():
    # æœ€å¥½çš„æºå½“ç„¶æ˜¯ https://github.com/trendingï¼Œä½†æ˜¯éœ€è¦è§£æ
    now = get_now()
    docs = await parse_github_trending()

    # ä¼˜å…ˆæ•°ç»„
    priority_docs = []
    not_priority_docs = []

    for doc in docs:
        name = doc['name']
        item = get_github_trending_by_name(name)
        if item:
            print('update',item['name'])
            # update item
            for k in doc.keys():
                item[k] = doc[k]
            last_pushtime = item['last_pushtime']
            now_date = now.date()
            last_pushtime_date = last_pushtime.date()
            difference = abs(now_date - last_pushtime_date)
            if difference >= timedelta(days=2):
                # è¶…è¿‡ä¸€å¤©ï¼Œä¼˜å…ˆæ¨é€
                item['times_of_today'] += 1
                item['last_pushtime'] = now
                priority_docs.append((difference.days,item))
            elif not item['summary']:
                priority_docs.append((1,item))
            else:
                not_priority_docs.append(item)
        else:
            print('new item',doc['name'])
            # update doc
            doc['about_zh'] = translate(doc['about']) if len(doc['about']) > 50 else doc['about']
            doc['summary'] = ''
            doc['description'] = ''
            doc['times_of_today'] = 1
            doc['last_pushtime'] = now
            priority_docs.append((100,doc))
        
    priority_docs = sorted(priority_docs, key=lambda x: x[0], reverse=True)
    priority_docs = [item[1] for item in priority_docs]

    # insert all docs back to sql
    print('insert github to sql')
    for doc in priority_docs:
        upsert_github_trending(doc)

    if len(priority_docs)== 0:
        return False


    # get info by crawling
    for doc in priority_docs + not_priority_docs:
        name = doc['name']
        if not doc['summary']:
            try:
                logging.info(f'çˆ¬å– {name}çš„å†…å®¹')
                content = await get_github_content(name)
            except:
                traceback.print_exc()
                content = ''
            if not content:
                continue
            logging.info(f'çˆ¬å– {name}çš„å†…å®¹æˆåŠŸ, ç”Ÿæˆæ‘˜è¦')
            try:
                summary, usage = article_summary({'title':name,'content':content},do_split=True)
                if usage:
                    insert_token_usage(usage,UseCase.GITHUB,'global')
                doc['summary'] = summary['summary']
                doc['keywords'] = summary['keywords']
                doc["category"] = summary['category']
                doc['description'] = content
                upsert_github_trending(doc)
            except:
                traceback.print_exc()
                continue

    return True

def get_arxiv_by_category(
        category=['cs.CL','cs.AI'], 
        max_results=200, 
        topics = 'Anything that related to language models, LLM, agent, GPT, llama, multi agent, multimodal, or using language models to solve problems',
        non_topics = 'å°è¯­ç§å¦‚æ³•è¯­ã€å¾·è¯­ã€æ—¥è¯­ç­‰éä¸­è‹±æ–‡æ–¹é¢çš„ç ”ç©¶å†…å®¹ï¼›å‘½åå®ä½“è¯†åˆ«'
        ):
    search = arxiv.Search(
        query = ' OR '.join(f"cat:{c}" for c in category),
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
        )
    
    tasks = []
    logging.info('æŸ¥è¯¢arxivè®ºæ–‡')
    for result in search.results():
        if result.primary_category not in category:
            continue
        entry_id = result.entry_id
        doc = get_arxiv_by_id(entry_id)
        if doc:
            continue
        tasks.append(result)
    logging.info(f'å…±{len(tasks)}ç¯‡è®ºæ–‡')

    def task_process(result):
        paper = '''## Title: \n{title}\n\n## Abstract:\n{summary}\n\nè¯·è®°ä½æˆ‘çš„å…´è¶£æ˜¯ï¼š{topics}.\næˆ‘ä¸æ„Ÿå…´è¶£çš„æ˜¯ï¼š{non_topics}\nè¯·ä½ ä¸¥æ ¼æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦æå–ä¸»é¢˜è¯ï¼Œä¸è¦æé€ ï¼Œå¹¶ä¸”ä¸è¦ä½¿ç”¨æˆ‘æä¾›çš„å…´è¶£è¯ã€‚\nåªè¾“å‡ºjsonå†…å®¹ï¼Œä¸è¦ä»»ä½•è§£é‡Šå’Œè¯´æ˜ã€‚'''.format(title=result.title,summary = result.summary[:3000], topics=topics,non_topics=non_topics)    
        entry_id = result.entry_id
        publish_time = result.published
        messages = Messages([SystemMessage(ARXIV_SYSTEM),
                    UserMessage(paper)])
        try:
            res,usage = call_llm(messages,max_tokens=-1,temperature=0.1)
            if usage:
                insert_token_usage(
                    usage=usage,
                    use_case=UseCase.ARXIV,
                    user="global")
            res = str(res)
            res = try_extract_json(res)
            if not res or not res.get('summary',''):
                return {
                    'category': result.primary_category,
                    'comment': result.comment,
                    'authors': ','.join(x.name for x in result.authors[:3]),
                    'status': False,
                    'entry_id':entry_id,
                    'publish_time':publish_time,
                    'title':result.title,
                    'title_chinese': '',
                    'summary':result.summary,
                    'topic':'',
                    'matched': False,
                    "emoji": "ğŸ“‘",
                    'simple_summary':''
                    }
            return {
                'category': result.primary_category,
                'comment': result.comment,
                'authors': ','.join(x.name for x in result.authors[:3]),
                'status': True,
                'entry_id':entry_id,
                'publish_time':publish_time,
                'title':result.title,
                'title_chinese': res['title'],
                'summary':result.summary,
                'topic':res['topic'],
                'matched': bool(res['matched']),
                "emoji": "ğŸ“‘",
                'simple_summary':res['summary']}
        except:
            return {   
                'category': result.primary_category,
                'comment': result.comment,
                'authors': ','.join(x.name for x in result.authors[:3]),
                'status': False,
                'entry_id':entry_id,
                'publish_time':publish_time,
                'title':result.title,
                'title_chinese': '',
                'summary':result.summary,
                'topic':'',
                'matched': False,
                "emoji": "ğŸ“‘",
                'simple_summary':''
                }
        

    logging.info('æå–è®ºæ–‡æ‘˜è¦...')
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        # ä½¿ç”¨ map æ¥å¹¶å‘åœ°å¯¹æ¯ä¸ªä»»åŠ¡è°ƒç”¨ call_gpt å‡½æ•°
        ret = executor.map(task_process, tasks)

    fails = []
    docs = []
    for doc in ret:
        doc['crawl_time'] = get_now()
        if doc['status']:
            if doc['matched']:
                docs.append(doc)
            else:
                fails.append(doc)
            insert_arxiv(doc)
        else:
            fails.append(doc)
    logging.info(f'å…±å¤„ç†{len(docs)}ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­å¤±è´¥{len(fails)}ï¼ŒæˆåŠŸ{len(docs)}')

    if len(docs) < 1:
        return f"ä»Šå¤©æ²¡æœ‰æ›´æ–°çš„è®ºæ–‡äº†"

    return 'è®ºæ–‡å·²ç»æ›´æ–°'

def push_manage(from_wxid, msg, room):
    if msg == 'è®¢é˜…åŠŸèƒ½':
        default_msg = f'''
ã€Œç›´æ¥æ¨èå…¬ä¼—å·ç»™æˆ‘ï¼Œæˆ‘ä¼šè‡ªåŠ¨å¸®æ‚¨è¿›è¡Œå…³æ³¨ï¼Œå…³æ³¨ä¸Šé™ä¸º{MAX_PUBLIC_NUM}ã€
ã€Œå–æ¶ˆå…³æ³¨ å…¬ä¼—å·åç§°ã€å–æ¶ˆå…³æ³¨æŸä¸ªå…¬ä¼—å·
ã€Œå…¬ä¼—å·åˆ—è¡¨ã€è·å–ç›®å‰å…³æ³¨çš„å…¬ä¼—å·åˆ—è¡¨
ã€Œè®¢é˜…çŠ¶æ€ã€è·å–æ‚¨çš„è®¢é˜…é…ç½®ä¿¡æ¯ï¼Œå¦‚æ—¶é—´ï¼Œé‚®ç®±
ã€Œä¸»åŠ¨æ¨é€ã€ä¸»åŠ¨æ¨é€å…¬ä¼—å·æ¶ˆæ¯
ã€Œå–æ¶ˆä¸»åŠ¨æ¨é€ã€å–æ¶ˆä¸»åŠ¨æ¨é€å…¬ä¼—å·æ¶ˆæ¯
'''
        return default_msg
    
    info = get_user_subscription_info(from_wxid)
    if msg == 'è®¢é˜…çŠ¶æ€':
        if not info:
            return 'ä½ è¿˜æ²¡æœ‰é…ç½®è®¢é˜…ä¿¡æ¯ï¼Œå‚è€ƒã€Œè®¢é˜…åŠŸèƒ½ã€è¿›è¡Œé…ç½®'
        else:
            return f"ä½ çš„è®¢é˜…ä¿¡æ¯å¦‚ä¸‹ï¼š\n{info}"
    
    if msg == 'ä¸»åŠ¨æ¨é€':
        update_user_push_status(room if room else from_wxid, True)
        return 'å·²ä¸ºæ‚¨å¼€å¯ä¸»åŠ¨æ¨é€'
    
    if msg == 'å–æ¶ˆä¸»åŠ¨æ¨é€':
        update_user_push_status(room if room else from_wxid, False)
        return 'å·²ä¸ºæ‚¨å…³é—­ä¸»åŠ¨æ¨é€'

    if msg == 'å…¬ä¼—å·åˆ—è¡¨':
        pub_ids = get_user_all_pub_id(room if room else from_wxid)
        info = '\n'.join([f"{i} - {v['pub_name']}" for i,v in enumerate(pub_ids.values())])
        return "æ‚¨å…³æ³¨çš„å…¬ä¼—å·åˆ—è¡¨å¦‚ä¸‹:\n" + info

    if msg[:4] == 'å–æ¶ˆå…³æ³¨' and len(msg) > 4:
        pub_name = msg[4:].strip()
        if len(pub_name) >= 20:
            return 'è¯·æ£€æŸ¥å…¬ä¼—å·åç§°æ˜¯å¦æ­£ç¡®'
        ret = delete_user_pub_by_name(room if room else from_wxid, pub_name)
        return f"å–æ¶ˆå…³æ³¨å…¬ä¼—å·{pub_name}æˆåŠŸ" if ret else f"å–æ¶ˆå…³æ³¨å…¬ä¼—å·{pub_name}å¤±è´¥"

async def pub_manage(data, from_wxid, pub_name=''):
    msg = data['raw_msg']
    msg = xmltodict.parse(msg)
    items = msg['msg']['appmsg']['mmreader']['category']['item']
    # æœ‰äº›å…¬ä¼—å·æœ‰æ—¶å€™åªæœ‰ä¸€æ¡æ¨é€
    if not isinstance(items, list):
        items = [items]
    
    info = f'è·å–åˆ°å…¬ä¼—å·ã€Œ{pub_name}ã€ - {len(items)}æ¡æ¨é€'
    logging.info(info)
    if len(items) >= 10:
        info  = 'æ¨é€è¿‡å¤šï¼Œè§£ææœ‰é—®é¢˜ã€‚'
        logging.warning(info)
        return info

    docs = []
    for item in items:
        doc = await get_article_from_pub(item, from_wxid, pub_name)
        if doc:
            insert_wx_article_to_sql(doc)
            docs.append(doc)
    info += f'\næˆåŠŸè§£æ{len(docs)}æ¡å…¥åº“ã€‚'
    return info, docs
  
if __name__ == '__main__':
    get_arxiv_by_category(max_results=60)


